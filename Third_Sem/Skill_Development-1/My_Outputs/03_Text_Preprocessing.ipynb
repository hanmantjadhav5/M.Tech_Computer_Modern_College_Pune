{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Practical Assignment 4 - Text Preprocessing\n",
        "# ------------------------------------------------------------\n",
        "# Objective: Perform tokenization, stemming, and stop-word removal\n",
        "# along with other cleaning steps on text data."
      ],
      "metadata": {
        "id": "aFhDm9n7QePr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTpLRhThHkjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23548e3-1771-485d-8afb-eb0bdca34280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (2.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Step 1: Import Libraries\n",
        "!pip install emoji\n",
        "\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Step 2: Sample Text Dataset (can be replaced with any dataset)\n",
        "texts = [\n",
        "    \"Wow!!! This movie was AMAZING üòçüòç Visit https://imdb.com for more info.\",\n",
        "    \"I didn't like the film... It was boring and too long!!! #wasteoftime\",\n",
        "    \"Machine Learning is improving automation in industries. 100% effective!\",\n",
        "    \"The actors were great, but the story was average. üôÑ\",\n",
        "]\n",
        "\n",
        "print(\"Original Texts:\\n\")\n",
        "for t in texts:\n",
        "    print(\"-\", t)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zPhADq_RMyw",
        "outputId": "256f3aa1-f79e-4972-e666-5616fd404d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Texts:\n",
            "\n",
            "- Wow!!! This movie was AMAZING üòçüòç Visit https://imdb.com for more info.\n",
            "- I didn't like the film... It was boring and too long!!! #wasteoftime\n",
            "- Machine Learning is improving automation in industries. 100% effective!\n",
            "- The actors were great, but the story was average. üôÑ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Step 3: Define Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Original Text:\", text)\n",
        "\n",
        "    # 1. Text Cleaning - remove URLs, emojis, hashtags, etc.\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = emoji.replace_emoji(text, replace='')  # Remove emojis\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n",
        "\n",
        "    # 2. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Remove punctuation and numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 4. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 5. Stop-word Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # 6. Stemming\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_tokens = [ps.stem(w) for w in filtered_tokens]\n",
        "\n",
        "    # 7. Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "\n",
        "    # Display intermediate outputs\n",
        "    print(\"Cleaned Text:\", text)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Filtered (No Stop-words):\", filtered_tokens)\n",
        "    print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "    print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "    # Return processed text\n",
        "    return \" \".join(lemmatized_tokens)\n"
      ],
      "metadata": {
        "id": "ddPul1omHtik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Step 4: Apply Preprocessing\n",
        "cleaned_texts = [preprocess_text(t) for t in texts]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwps-nb0HyJ2",
        "outputId": "bbcdd347-1ec7-4112-a05e-986aa5997f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Original Text: Wow!!! This movie was AMAZING üòçüòç Visit https://imdb.com for more info.\n",
            "Cleaned Text: wow this movie was amazing  visit  for more info\n",
            "Tokens: ['wow', 'this', 'movie', 'was', 'amazing', 'visit', 'for', 'more', 'info']\n",
            "Filtered (No Stop-words): ['wow', 'movie', 'amazing', 'visit', 'info']\n",
            "Stemmed Tokens: ['wow', 'movi', 'amaz', 'visit', 'info']\n",
            "Lemmatized Tokens: ['wow', 'movie', 'amazing', 'visit', 'info']\n",
            "\n",
            "==============================\n",
            "Original Text: I didn't like the film... It was boring and too long!!! #wasteoftime\n",
            "Cleaned Text: i didnt like the film it was boring and too long \n",
            "Tokens: ['i', 'didnt', 'like', 'the', 'film', 'it', 'was', 'boring', 'and', 'too', 'long']\n",
            "Filtered (No Stop-words): ['didnt', 'like', 'film', 'boring', 'long']\n",
            "Stemmed Tokens: ['didnt', 'like', 'film', 'bore', 'long']\n",
            "Lemmatized Tokens: ['didnt', 'like', 'film', 'boring', 'long']\n",
            "\n",
            "==============================\n",
            "Original Text: Machine Learning is improving automation in industries. 100% effective!\n",
            "Cleaned Text: machine learning is improving automation in industries  effective\n",
            "Tokens: ['machine', 'learning', 'is', 'improving', 'automation', 'in', 'industries', 'effective']\n",
            "Filtered (No Stop-words): ['machine', 'learning', 'improving', 'automation', 'industries', 'effective']\n",
            "Stemmed Tokens: ['machin', 'learn', 'improv', 'autom', 'industri', 'effect']\n",
            "Lemmatized Tokens: ['machine', 'learning', 'improving', 'automation', 'industry', 'effective']\n",
            "\n",
            "==============================\n",
            "Original Text: The actors were great, but the story was average. üôÑ\n",
            "Cleaned Text: the actors were great but the story was average \n",
            "Tokens: ['the', 'actors', 'were', 'great', 'but', 'the', 'story', 'was', 'average']\n",
            "Filtered (No Stop-words): ['actors', 'great', 'story', 'average']\n",
            "Stemmed Tokens: ['actor', 'great', 'stori', 'averag']\n",
            "Lemmatized Tokens: ['actor', 'great', 'story', 'average']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Step 4: Apply Preprocessing\n",
        "cleaned_texts = [preprocess_text(t) for t in texts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NxZ2qKVH67h",
        "outputId": "9165bf02-813f-4ad2-f1af-9b12c8d99534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Original Text: Wow!!! This movie was AMAZING üòçüòç Visit https://imdb.com for more info.\n",
            "Cleaned Text: wow this movie was amazing  visit  for more info\n",
            "Tokens: ['wow', 'this', 'movie', 'was', 'amazing', 'visit', 'for', 'more', 'info']\n",
            "Filtered (No Stop-words): ['wow', 'movie', 'amazing', 'visit', 'info']\n",
            "Stemmed Tokens: ['wow', 'movi', 'amaz', 'visit', 'info']\n",
            "Lemmatized Tokens: ['wow', 'movie', 'amazing', 'visit', 'info']\n",
            "\n",
            "==============================\n",
            "Original Text: I didn't like the film... It was boring and too long!!! #wasteoftime\n",
            "Cleaned Text: i didnt like the film it was boring and too long \n",
            "Tokens: ['i', 'didnt', 'like', 'the', 'film', 'it', 'was', 'boring', 'and', 'too', 'long']\n",
            "Filtered (No Stop-words): ['didnt', 'like', 'film', 'boring', 'long']\n",
            "Stemmed Tokens: ['didnt', 'like', 'film', 'bore', 'long']\n",
            "Lemmatized Tokens: ['didnt', 'like', 'film', 'boring', 'long']\n",
            "\n",
            "==============================\n",
            "Original Text: Machine Learning is improving automation in industries. 100% effective!\n",
            "Cleaned Text: machine learning is improving automation in industries  effective\n",
            "Tokens: ['machine', 'learning', 'is', 'improving', 'automation', 'in', 'industries', 'effective']\n",
            "Filtered (No Stop-words): ['machine', 'learning', 'improving', 'automation', 'industries', 'effective']\n",
            "Stemmed Tokens: ['machin', 'learn', 'improv', 'autom', 'industri', 'effect']\n",
            "Lemmatized Tokens: ['machine', 'learning', 'improving', 'automation', 'industry', 'effective']\n",
            "\n",
            "==============================\n",
            "Original Text: The actors were great, but the story was average. üôÑ\n",
            "Cleaned Text: the actors were great but the story was average \n",
            "Tokens: ['the', 'actors', 'were', 'great', 'but', 'the', 'story', 'was', 'average']\n",
            "Filtered (No Stop-words): ['actors', 'great', 'story', 'average']\n",
            "Stemmed Tokens: ['actor', 'great', 'stori', 'averag']\n",
            "Lemmatized Tokens: ['actor', 'great', 'story', 'average']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Step 5: Results\n",
        "print(\"\\n\\nFinal Preprocessed Texts:\\n\")\n",
        "for i, ct in enumerate(cleaned_texts, 1):\n",
        "    print(f\"{i}. {ct}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vghBromiH-_k",
        "outputId": "89c14858-7cf4-4e2a-bab4-fcd6eaeafda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Final Preprocessed Texts:\n",
            "\n",
            "1. wow movie amazing visit info\n",
            "2. didnt like film boring long\n",
            "3. machine learning improving automation industry effective\n",
            "4. actor great story average\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Step 6: Analysis and Conclusion\n",
        "print(\"\\nAnalysis:\")\n",
        "print(\"1. Text cleaning removed unwanted URLs, emojis, and punctuation effectively.\")\n",
        "print(\"2. Stop-word removal and stemming reduced text size while keeping meaning intact.\")\n",
        "print(\"3. Lemmatization provided proper word forms, making text more uniform for modeling.\")\n",
        "\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"Text preprocessing effectively transforms raw, unstructured text into clean, analyzable form, improving model performance in NLP tasks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qOriE23IGPL",
        "outputId": "bfc4d211-7eca-4557-c85a-b52994f24b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis:\n",
            "1. Text cleaning removed unwanted URLs, emojis, and punctuation effectively.\n",
            "2. Stop-word removal and stemming reduced text size while keeping meaning intact.\n",
            "3. Lemmatization provided proper word forms, making text more uniform for modeling.\n",
            "\n",
            "Conclusion:\n",
            "Text preprocessing effectively transforms raw, unstructured text into clean, analyzable form, improving model performance in NLP tasks.\n"
          ]
        }
      ]
    }
  ]
}