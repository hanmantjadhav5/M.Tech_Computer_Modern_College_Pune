# -*- coding: utf-8 -*-
"""MFCS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xUw97oNP6equFTEMny8F6DKkwCX5yCBI
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Step 1: Generate continuous data
def generate_data(n_samples=500):
    np.random.seed(42)
    # Class 0
    class_0_feature_1 = np.random.normal(loc=5, scale=2, size=n_samples//2)
    class_0_feature_2 = np.random.normal(loc=3, scale=1, size=n_samples//2)
    class_0_labels = np.zeros(n_samples//2)

    # Class 1
    class_1_feature_1 = np.random.normal(loc=10, scale=2, size=n_samples//2)
    class_1_feature_2 = np.random.normal(loc=7, scale=1.5, size=n_samples//2)
    class_1_labels = np.ones(n_samples//2)

    # Combine data
    features = np.vstack((np.column_stack((class_0_feature_1, class_0_feature_2)),
                          np.column_stack((class_1_feature_1, class_1_feature_2))))
    labels = np.concatenate((class_0_labels, class_1_labels))

    return features, labels

# Step 2: Split data into training and testing sets
features, labels = generate_data()
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42)

# Step 3: Naive Bayes Classifier
class NaiveBayesClassifier:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = {}
        self.variance = {}
        self.priors = {}

        for cls in self.classes:
            X_cls = X[y == cls]
            self.mean[cls] = np.mean(X_cls, axis=0)
            self.variance[cls] = np.var(X_cls, axis=0)
            self.priors[cls] = len(X_cls) / len(X)

    def _gaussian_pdf(self, x, mean, var):
        eps = 1e-6  # To avoid division by zero
        coeff = 1 / np.sqrt(2 * np.pi * var + eps)
        exponent = np.exp(-(x - mean)**2 / (2 * var + eps))
        return coeff * exponent

    def predict(self, X):
        predictions = []
        for x in X:
            posteriors = []
            for cls in self.classes:
                prior = np.log(self.priors[cls])
                conditional = np.sum(np.log(self._gaussian_pdf(x, self.mean[cls], self.variance[cls])))
                posterior = prior + conditional
                posteriors.append(posterior)
            predictions.append(self.classes[np.argmax(posteriors)])
        return np.array(predictions)

# Step 4: Train the model
model = NaiveBayesClassifier()
model.fit(X_train, y_train)

# Step 5: Make predictions
y_pred = model.predict(X_test)

# Step 6: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")